get_face_68_key_point.py 技术细节

首先创建用于人脸存储数据的文件夹(my_faces)
然后读取视频文件(Test.mp4),这个视频是我用手机前置摄像头录制的，记录了各种鬼脸表情。
我没有提供这个视频，长得太丑了，你们要是想训练就自己录吧～～

这里面会把截取到的图像稍微处理一下，然后用dlib检测关键点信息，我写了一个函数，db.get_68_key_point
这个函数在dlib_68_key_point.py文件里被定义。

获取完数据后就直接保存进pkl文件里面，当然我这里也没有提供，你们直接一运行就可以得出自己的pkl文件了。
保存并序列化数据用的是pickle这个库，还是蛮好用的，open文件必须用二进制模式，无论读取还是写入都是哦～～


dlib_68_key_point.py 技术细节
这里用到了一个文件shape_predictor_68_face_landmarks.dat 90多Mb，从网上能直接下载到，我就不上传了。
这里面只有一个函数，输入图像，输出一个列表，是图像和68点数据的集合。
值得一提的是，这里我用numpy把68点数据都变成了1维数据，这样是方便以后进入训练，你也可以不这样做。
图片是有几率识别不出来的哦，这里我没加判定，而是在之后的步骤加的判断，(这个文件的数据是否正常)
如果一个都识别不出来可能是你的图片太大了。


show_68_key_data.py 技术细节
我建了一个简单的类，用于查看上一步保存好的pkl是否正常，当然也有一些失效的(未识别出来的)数据，因为我上一步保存
的时候没加判定嘛，所以这里得用try判定一下即可。
当然你也可以修改成只显示某一份数据。
这个文件没啥用，就是看效果用的。
dlib 68点的方法网上一搜就能搜出来，我这个就是照着改了一下。


show_dlib68_key.py 技术细节
这个其实不应该添加进来，因为它是一个独立的文件，并没有参与进整个cnn的流程之中。
打开摄像头，用dlib识别68点把识别出来的，实时显示出来。
如果不卡的话说明你电脑没啥问题哦～
这个同样也是稍微改了一下网上的dlib 68点的教程


save_data_tensor.py 技术细节
这个文件里面我注释了一部分，详情需要你们去找一下tensorflow的数据存储格式。
argparse这个库比较方便，大概就是属性的管理吧，相当于添加了一个遥控器。
这样存储的好处肯定是比你自己保存强多了，值得注意的就是这些保存的步骤，需要每一步都去网上查一下，
大概懂是什么意思。
这里边有一些路径需要改一下。最后数据会存到tfrecord这个文件夹里面。

train_module.py 技术细节
想要训练网络的话就直接跑这个文件，最上面三个模块应该是为了python2和python3不同而准备的，反正我没怎么用到，
你们可以试一下不加这玩意有没有问题。
这里面用到的都是tensorflow的比较高级的方法，更省事一点而已。
最下面的tf.app.run()要是想换成main()，你就的把上面main()函数里面的_符号先删掉，这样就不会报错啦。

开始时初始化所有变量tf.logging.set_verbosity(tf.logging.INFO)
然后你要是数据不多的话其实可以把保存的频率适当的修改一下，这里面其他属性也可以适当修改。
main函数里面先实例化了另一个文件ModelConfig.py 这个就是网络的总遥控器，里面有很多属性。
如果你的学习不顺利的话也可以在这里直接修改学习率。
一旦开始训练得话，会自动保存，所以死机什么的不用怕，肝就完事了～

inference_module.py 技术细节
这个就是照着train_module.py改的，当你训练完模型后就跑这个，可以测试网络的性能，就这个网络而言，其实会
欠拟合，具体啥问题我以后再解决，总之效果很差就对了，当然也不用灰心，这这是一个练手的项目，没准以后学到了
新知识，就能回过头来解决了。
我问别人，说可以试试添加bn层，或者用LSTM增强时序关联性。你们要是有能力的话可以试试。
我在下一个版本会尝试一下。
这里面numpy旋转90度也是因为的手机录视频的原因嘛，你要是视频本身就是正的，那就不用这一步了。


ModelConfig.py 技术细节
这里面的image和v其实就类似与字典的key
分别是图片和68点数据。

下面predict_num是得看报错的，要是报错的话可能就是因为你我的图片进入格式不同
导致矩阵运算格式不正确。
key_num，这个是68点的横纵坐标数据的总数。

image_height，image_width，都得自己改一改

file_pattern保存着数据集
train_dir会输出训练好的模型

batch_size你要是电脑好的话可以改成32，
这个是每次训练的批次数量，不明白的直接百度就能搜到。
其他的暂时还没研究，想要知道的也可以百度

VisualModule.py 技术细节
这个就是网络的全貌了，分为训练和工作两种模式。
我都注释了，这网络模型看不懂就亲手做一遍，其实还是比较差的。
更详细的可以自己慢慢研究，也有好处，可以加深理解嘛～
核心思想就是输入图片，输出68点数据。
但是这样其实每张照片都不关联，视频的时序没有体现出来，在后面的版本我会改进这里的输入输出的。
我猜测，单纯的更改输入和输出的形式其实也能优化神经网络。
比如，输出也是一张图片，只不过扣去68点所在的像素。这样直觉上就会比现在这样好一些。